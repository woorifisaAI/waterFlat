{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/crawling.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pvbu_hsyEgz",
        "outputId": "397c09b2-418b-45bc-a482-d1aaa190b87b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/crawling.zip\n",
            "   creating: 공식사이트/\n",
            "  inflating: 공식사이트/국민은행공식인스타_903  \n",
            "  inflating: 공식사이트/기업은행공식인스타  \n",
            "  inflating: 공식사이트/농협은행공식인스타  \n",
            "  inflating: 공식사이트/마케팅_259  \n",
            "  inflating: 공식사이트/신한은행공식인스타  \n",
            "  inflating: 공식사이트/우리금융공식인스타_400  \n",
            "  inflating: 공식사이트/우리은행공식인스타_899  \n",
            "  inflating: 공식사이트/하나금융공식인스타  \n",
            "   creating: 태그/\n",
            "  inflating: 태그/국민은행.csv  \n",
            "  inflating: 태그/기업은행.csv  \n",
            "  inflating: 태그/농협은행.csv  \n",
            "  inflating: 태그/산업은행.csv  \n",
            "  inflating: 태그/신한은행.csv  \n",
            "  inflating: 태그/우리은행.csv  \n",
            "  inflating: 태그/은행홍보문구.xlsx  \n",
            "  inflating: 태그/인스타그램 주소에서 크롤링.ipynb  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRcxIIVXxMjG",
        "outputId": "7a9618cf-9e0a-40c2-9d69-e3ccb88a6738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "import re\n",
        "import fastai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mVIPtHbx58N",
        "outputId": "679ee229-817d-4896-b370-f846a640158d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/251.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/251.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "files_path = os.path.abspath(\"/content/공식사이트\")\n",
        "files=os.listdir(files_path)\n",
        "\n",
        "data_path=[]\n",
        "for file in files:\n",
        "    file_path=os.path.join(files_path,file)\n",
        "    data_path.append(file_path)\n",
        "\n",
        "print(data_path)\n",
        "for file in data_path:\n",
        "    with open(file, encoding='utf-8') as f:\n",
        "        with open('preprocessed_ori.txt','w', encoding='utf-8')as wsw:\n",
        "          with open('preprocessed_regex.txt','w', encoding='utf-8')as w:\n",
        "              data = (f.readlines())\n",
        "              for item in data:\n",
        "                wsw.write(item)\n",
        "\n",
        "              for str_ in data:\n",
        "                  sub = (str_.split(','))\n",
        "                  clean_text = re.sub(r'[^\\w\\s]', '', sub[1])\n",
        "                  w.write(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3CDv8HHySuL",
        "outputId": "2a1993e7-2ae3-4a92-deac-2f460c0d74d4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/공식사이트/국민은행공식인스타_903', '/content/공식사이트/우리금융공식인스타_400', '/content/공식사이트/기업은행공식인스타', '/content/공식사이트/농협은행공식인스타', '/content/공식사이트/신한은행공식인스타', '/content/공식사이트/마케팅_259', '/content/공식사이트/우리은행공식인스타_899', '/content/공식사이트/하나금융공식인스타']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
        "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
        ")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#   'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
        "#   pad_token_id=tokenizer.eos_token_id,\n",
        "#   torch_dtype='auto', low_cpu_mem_usage=True\n",
        "# ).to(device='cuda', non_blocking=True)\n",
        "# _ = model.eval()\n",
        "\n",
        "# prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n",
        "# with torch.no_grad():\n",
        "#   tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
        "#   gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n",
        "#   generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "# print(generated)  # print: 인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 문제의 해답을 찾을 수 있을 것이다. 과학기술이 고도로 발달한 21세기를 살아갈 우리 아이들에게 가장 필요한 것은 사고력 훈련이다. 사고력 훈련을 통해, 세상\n"
      ],
      "metadata": {
        "id": "M4p17WaRxbRh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pattern = r'\\([^)]*\\)'  # ()\n",
        "pattern1 = r'\\[[^)]*\\]'  # []\n",
        "pattern2 = r'\\<[^)]*\\>'  # <>\n",
        "pattern3 = r'\\{[^)]*\\}'  # {}\n",
        "pattern4 = r'\\【[^)]*\\】' # 【】\n",
        "pattern5 = r'\\❝[^)]*\\❞' #  ❝ ❞\n",
        "\n",
        "pattern_list=[pattern1,pattern2,pattern3,pattern4,pattern5]\n",
        "\n",
        "# data['content'] = data['content'].apply(lambda x: re.sub(pattern, '', x))\n",
        "# data['content'] = data['content'].apply(lambda x: re.sub(pattern1, '', x))\n",
        "# data['content'] = data['content'].apply(lambda x: re.sub(pattern2, '', x))\n",
        "# data['content'] = data['content'].apply(lambda x: re.sub(pattern3, '', x))\n",
        "# data['content'] = data['content'].apply(lambda x: re.sub(pattern4, '', x))\n",
        "# data['content'] = data['content'].apply(lambda x: re.sub(pattern5, '', x))\n",
        "# # data['content'] = data['content'].apply(lambda x: re.sub(pattern6, '', x))\n",
        "# data['content_split'] = data['content'].apply(lambda x: x.split()) #공백 기준으로 분리\n",
        "\n",
        "with open('preprocessed_ori.txt','r', encoding='utf-8')as f:\n",
        "  with open('preprocessed_nogwalho.txt','w', encoding='utf-8')as w:\n",
        "    data=f.readlines()\n",
        "    for str_ in data:\n",
        "        sub = (str_.split(','))\n",
        "        # str_ = re.sub(pattern, '', sub[1])\n",
        "        str_ = re.sub(r'[\\(\\)\\{\\}\\[\\]\\<\\>]', '', sub[1])\n",
        "        # for p in pattern_list:\n",
        "        #   str_ = re.sub(p, '', str_)\n",
        "        str_temp = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]', '@@@@@', str_)\n",
        "        if '@@@@@' in str_temp:\n",
        "          continue\n",
        "        w.write(str_)\n",
        ""
      ],
      "metadata": {
        "id": "exr10GHt62JW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str_ = '---------------------------------'\n",
        "\n",
        "print(str_.replace('--', \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQvgIRiPnzUf",
        "outputId": "0c77204a-e6d4-4468-c97b-959a9feb60d9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('preprocessed_nogwalho.txt','r', encoding='utf-8')as f:\n",
        "  with open('preprocessed_final.txt','w', encoding='utf-8')as w:\n",
        "    data=f.readlines()\n",
        "\n",
        "    for str_ in data:\n",
        "        # print(str_)\n",
        "        sub = (str_.replace('⁉', '?'))\n",
        "        sub = (sub.replace('--', ''))\n",
        "        # sub = (str_)\n",
        "        clean_text = re.sub(r'[^가-힣!-~\\s]', '', sub)\n",
        "        clean_text = re.sub(r'[\\\"\\']', '', clean_text)\n",
        "        w.write(clean_text)\n"
      ],
      "metadata": {
        "id": "7KD3G_KV2I3B"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('preprocessed_final.txt','r', encoding='utf-8')as f:\n",
        "  with open('preprocessed_result.txt','w', encoding='utf-8')as w:\n",
        "    data=f.readlines()\n",
        "    for item in data:\n",
        "      tmp=[]\n",
        "      for word in item.split():\n",
        "        if ('#' in word) or ('@' in word):\n",
        "          continue\n",
        "        tmp.append(word)\n",
        "      str_=' '.join(tmp)+'\\n'\n",
        "      w.write(str_)"
      ],
      "metadata": {
        "id": "VqFJSs8vyJ1L"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('preprocessed_result.txt','r', encoding='utf-8')as f:\n",
        "  with open('preprocessed_word_result.txt','w', encoding='utf-8')as w:\n",
        "    data=f.readlines()\n",
        "    for str_ in data:\n",
        "        sub = (str_)\n",
        "        clean_text = re.sub(r'https?://\\S+|www\\.\\S+', '', sub)\n",
        "        clean_text_final = re.sub(r'bit\\.\\S+', '', clean_text)\n",
        "        clean_text_real_final = re.sub(r'Bit\\.\\S+', '', clean_text_final)\n",
        "        # clean_text = re.sub(r'[\\\"\\']', '', clean_text)\n",
        "        w.write(clean_text_real_final)"
      ],
      "metadata": {
        "id": "ohFGcEiUmPg7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.encode('난ㄱ ㅏ 끔눈물을 흘린 ㄷ ㅏ 보고ㅅ ㅓ를ㅆ ㅓ ㅇ ㅑ ㅎ ㅏ ㄱ ㅓ 든눈물 보고서 너도 같이쓰자', return_tensors='pt').to(device='cuda', non_blocking=True)\n",
        "generated = tokenizer.batch_decode(tokens, skip_special_tokens=False)[0]\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff08JeJIz6q2",
        "outputId": "28170ac9-1634-4c57-c723-1d00a49fcaf5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "난ㄱ ㅏ 끔눈물을 흘린 ㄷ ㅏ 보고ㅅ ㅓ를ㅆ ㅓ ㅇ ㅑ ㅎ ㅏ ㄱ ㅓ 든눈물 보고서 너도 같이쓰자\n"
          ]
        }
      ]
    }
  ]
}